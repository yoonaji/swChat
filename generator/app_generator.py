from dotenv import load_dotenv
import os
import grpc
import asyncio

from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from contextlib import asynccontextmanager
import generator_pb2
import generator_pb2_grpc

load_dotenv()
LLM_MODEL = os.getenv("LLM_MODEL", "gpt-4o-mini")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

class GeneratorServicer(generator_pb2_grpc.GeneratorServiceServicer):
    def __init__(self):
        self.llm = ChatOpenAI(
            model=LLM_MODEL,
            openai_api_key=OPENAI_API_KEY,
            temperature=0  
        )
        self.prompt = prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=(
            "ë„ˆëŠ” ëŒ€í•™ ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©ëŒ€í•™(ì†ŒìœµëŒ€)ì˜ ê·œì • ë° êµê³¼ê³¼ì • ë‹´ë‹¹ ì¡°êµì•¼.\n"
            "ì£¼ì–´ì§„ ìë£Œ(context) ë‚´ì˜ ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ë‹µë³€í•´.\n"
            "ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ì  ì§€ì‹ìœ¼ë¡œ ë‹µí•˜ì§€ ë§ˆ. ê·¼ê±°ê°€ ì—†ìœ¼ë©´ 'ìë£Œ ë‚´ì—ì„œ í™•ì¸ ë¶ˆê°€'ë¼ê³  ëª…ì‹œí•´.\n"
            "ë‹µë³€ ë§ˆì§€ë§‰ ì¤„ì—ëŠ” ë°˜ë“œì‹œ ê·¼ê±° ì¶œì²˜ë¥¼ [ì‹œíŠ¸ëª…(í–‰ë²ˆí˜¸)] í˜•íƒœë¡œ í‘œê¸°í•´.\n\n"
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
            "ã€ìë£Œã€‘\n{context}\n"
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
            "ã€ì§ˆë¬¸ã€‘\n{question}\n"
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
            "ã€ë‹µë³€ã€‘\n"
            "â€» ì°¸ê³ :\n"
            "- ì´ ë‹¨ê³¼ëŒ€ì˜ ê³µì‹ ëª…ì¹­ì€ 'ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©ëŒ€í•™'ì´ë©°, ì¤„ì—¬ì„œ 'ì†ŒìœµëŒ€'ë¼ê³  ë¶€ë¥¸ë‹¤.\n"
            "- ì†ŒìœµëŒ€ì—ëŠ” ì„¸ ê°œì˜ í•™ê³¼ê°€ ìˆìœ¼ë©°, ì•½ì¹­ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:\n"
            "  â€¢ ì»´í“¨í„°ê³µí•™ê³¼ â†’ 'ì»´ê³µ'\n"
            "  â€¢ ì¸ê³µì§€ëŠ¥í•™ê³¼ â†’ 'ì¸ì§€' ë˜ëŠ” 'ì¸ì§€ê³¼'\n"
            "  â€¢ ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©í•™ê³¼ â†’ 'ì†Œìœµ'\n"
            "- ë‹µë³€ ì‹œ 'ì†ŒìœµëŒ€'(ë‹¨ê³¼ëŒ€)ì™€ 'ì†Œìœµê³¼'(í•™ê³¼)ë¥¼ ë°˜ë“œì‹œ êµ¬ë¶„í•´ì„œ ì‚¬ìš©í•´."

            ),
        )
        self.chain = self.prompt | self.llm | StrOutputParser()
    
    async def Generate(self, request, context):
        query = request.query
        contexts = request.contexts
        
        context_str = "\n\n".join(contexts)
        
        response_text = await self.chain.ainvoke({
                "context": context_str,
                "question": query
            })
        
        
        return generator_pb2.GenerateResponse(answer=response_text)
            
 
async def serve():
    server = grpc.aio.server()
    generator_pb2_grpc.add_GeneratorServiceServicer_to_server(GeneratorServicer(), server)
    server.add_insecure_port('[::]:50052')
    print("ğŸš€ Generator gRPC Server running on port 50052")
    await server.start()
    await server.wait_for_termination()

if __name__ == "__main__":
    asyncio.run(serve())   


