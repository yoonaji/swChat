# app_generator.py

from fastapi import FastAPI, Query, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
import os, time
import httpx  # â­ï¸ ë‹¤ë¥¸ ì„œë²„ì™€ í†µì‹ í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (í•„ìˆ˜)

from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from contextlib import asynccontextmanager

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=(
         "ë„ˆëŠ” ëŒ€í•™ ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©ëŒ€í•™(ì†ŒìœµëŒ€)ì˜ ê·œì • ë° êµê³¼ê³¼ì • ë‹´ë‹¹ ì¡°êµì•¼.\n"
        "ì£¼ì–´ì§„ ìë£Œ(context) ë‚´ì˜ ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ë‹µë³€í•´.\n"
        "ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ì  ì§€ì‹ìœ¼ë¡œ ë‹µí•˜ì§€ ë§ˆ. ê·¼ê±°ê°€ ì—†ìœ¼ë©´ 'ìë£Œ ë‚´ì—ì„œ í™•ì¸ ë¶ˆê°€'ë¼ê³  ëª…ì‹œí•´.\n"
        "ë‹µë³€ ë§ˆì§€ë§‰ ì¤„ì—ëŠ” ë°˜ë“œì‹œ ê·¼ê±° ì¶œì²˜ë¥¼ [ì‹œíŠ¸ëª…(í–‰ë²ˆí˜¸)] í˜•íƒœë¡œ í‘œê¸°í•´.\n\n"
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
        "ã€ìë£Œã€‘\n{context}\n"
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
        "ã€ì§ˆë¬¸ã€‘\n{question}\n"
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
        "ã€ë‹µë³€ã€‘\n"
        "â€» ì°¸ê³ :\n"
        "- ì´ ë‹¨ê³¼ëŒ€ì˜ ê³µì‹ ëª…ì¹­ì€ 'ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©ëŒ€í•™'ì´ë©°, ì¤„ì—¬ì„œ 'ì†ŒìœµëŒ€'ë¼ê³  ë¶€ë¥¸ë‹¤.\n"
        "- ì†ŒìœµëŒ€ì—ëŠ” ì„¸ ê°œì˜ í•™ê³¼ê°€ ìˆìœ¼ë©°, ì•½ì¹­ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:\n"
        "  â€¢ ì»´í“¨í„°ê³µí•™ê³¼ â†’ 'ì»´ê³µ'\n"
        "  â€¢ ì¸ê³µì§€ëŠ¥í•™ê³¼ â†’ 'ì¸ì§€' ë˜ëŠ” 'ì¸ì§€ê³¼'\n"
        "  â€¢ ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©í•™ê³¼ â†’ 'ì†Œìœµ'\n"
        "- ë‹µë³€ ì‹œ 'ì†ŒìœµëŒ€'(ë‹¨ê³¼ëŒ€)ì™€ 'ì†Œìœµê³¼'(í•™ê³¼)ë¥¼ ë°˜ë“œì‹œ êµ¬ë¶„í•´ì„œ ì‚¬ìš©í•´."

    ),
)

load_dotenv()
RETRIEVER_API_URL = os.getenv("RETRIEVER_API_URL", "http://retriever:8001")
LLM_MODEL = os.getenv("LLM_MODEL", "gpt-4o-mini")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    raise RuntimeError("âŒ OPENAI_API_KEY not found in .env")

@asynccontextmanager
async def lifespan(app: FastAPI):
    # --- ì½”ë“œê°€ ì‹œì‘ë  ë•Œ ì‹¤í–‰ ---
    print(f"ğŸ”— [Generator] HTTP Client for {RETRIEVER_API_URL} ready.")
    app.state.httpx_client = httpx.AsyncClient() # ì™¸ë¶€ apië¥¼ í˜¸ì¶œí•´ì•¼í•  ë•Œ ë©ˆì¶”ì§€ ì•Šê³  ì„œë²„ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©
    
    yield  # ğŸ‘ˆ ì´ ì‹œì ì—ì„œ FastAPI ì•±ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.
    
    # --- ì½”ë“œê°€ ì¢…ë£Œë  ë•Œ ì‹¤í–‰ ---
    print("ğŸ›‘ [Generator] HTTP Client closed.")
    await app.state.httpx_client.aclose()

# --- FastAPI ì•± ì´ˆê¸°í™” ---
app = FastAPI(
  title="Generator Chatbot API",
  lifespan=lifespan
  )

# --- CORS ì„¤ì • (ë™ì¼) ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- LLM ì´ˆê¸°í™” ---
# Qdrant/Retriever ê´€ë ¨ ì½”ë“œëŠ” ì—¬ê¸°ì„œ ëª¨ë‘ ì‚­ì œ
try:
    print("ğŸ”— [Generator] Initializing LLM...")
    llm = ChatOpenAI(model=LLM_MODEL)
    print("âœ… [Generator] LLM initialized.")
except Exception as e:
    raise RuntimeError(f"ğŸš¨ [Generator] LLM Initialization failed: {e}")


# --- í—¬ìŠ¤ì²´í¬ ---
@app.get("/")
def home():
    return {"message": "âœ… Generator Chatbot API is running!"}

# --- â­ï¸ API ì—”ë“œí¬ì¸íŠ¸ (ë¡œì§ ë³€ê²½) ---
@app.get("/ask")
async def ask(query: str = Query(..., description="ì‚¬ìš©ì ì§ˆë¬¸")):
    """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ Retriever APIì™€ LLMì„ ìˆœì°¨ì ìœ¼ë¡œ í˜¸ì¶œí•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.
    """
    client = app.state.httpx_client # app.stateì— ì €ì¥í•´ ë‘ì—ˆë˜ httpx.AsyncClient ê°ì²´(object) ê·¸ ìì²´ê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤. ì •í™•íˆëŠ” ê·¸ ê°ì²´ë¥¼ ê°€ë¦¬í‚¤ëŠ” 'ì°¸ì¡°(reference)' ë˜ëŠ” **'ë©”ëª¨ë¦¬ ì£¼ì†Œ'**ê°€ ë³µì‚¬ë˜ì–´ client ë³€ìˆ˜ì— ì €ì¥
    
    # 1. â­ï¸ (í†µì‹  1) Retriever API í˜¸ì¶œ
    try:
        response = await client.get(
            f"{RETRIEVER_API_URL}/retrieve", 
            params={"query": query},
            timeout=20.0
        )
        response.raise_for_status() # HTTP ì˜¤ë¥˜ê°€ 200 OKê°€ ì•„ë‹ˆë©´ ì˜ˆì™¸ ë°œìƒ
        documents = response.json()
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°
        context = "\n\n".join([doc['page_content'] for doc in documents])
        # ì¶œì²˜(sources) ì •ë³´ íŒŒì‹±
        sources = [
            {
                "sheet": doc['metadata'].get("sheet"),
                "row_idx": doc['metadata'].get("row_idx"),
                "table_title": doc['metadata'].get("table_title"),
            }
            for doc in documents
        ]

    except httpx.RequestError as e:
        raise HTTPException(status_code=503, detail=f"Retriever API ({e.request.url}) í†µì‹  ì˜¤ë¥˜: {e}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Retriever API ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    # 2. â­ï¸ (í†µì‹  2) LLM API í˜¸ì¶œ (LangChain Chain ìˆ˜ë™ êµ¬ì„±)
    try:
        # RetrievalQA ëŒ€ì‹  ìˆ˜ë™ìœ¼ë¡œ ì²´ì¸ êµ¬ì„±
        rag_chain = (
            {"context": (lambda x: context), "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        # ë¹„ë™ê¸°ë¡œ LLM í˜¸ì¶œ
        answer = await rag_chain.ainvoke(query)
        
        return {
            "question": query,
            "answer": answer,
            "sources": sources # Retrieverì—ì„œ ë°›ì•„ì˜¨ ì¶œì²˜ ì •ë³´
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM API ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

# --- ì‹¤í–‰ (uvicornìš©) ---
if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ Starting Generator FastAPI on http://localhost:8000")
    uvicorn.run("app_generator:app", host="0.0.0.0", port=8000, log_level="info")